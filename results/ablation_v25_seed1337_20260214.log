============================================================
EMBER ABLATION STUDY (5 conditions)
============================================================

>>> Training Standard...
Device: cpu
Ember: 10.65M parameters (LIF=OFF)
[Standard] iter     0 | train 4.2882 | val 4.2811 | 15.9s
[Standard] iter   500 | train 1.8590 | val 1.9779 | 675.0s
[Standard] iter  1000 | train 1.3780 | val 1.6036 | 1304.6s
[Standard] iter  1500 | train 1.2529 | val 1.5278 | 1933.8s
[Standard] iter  1999 | train 1.1698 | val 1.4923 | 2561.5s

============================================================
[Standard] Training complete in 2562.8s
[Standard] Best val loss: 1.4923

--- Generated sample ---

KING EDWARD IV:
Who she 'twere the sorrow hands again.

GLOUCESTER:
Where shall I show thee? I am not too much.

LADY GREY:
Then, I have done to make my sons and weep.

LADY GREY:
Do you seem the sight, he looks upon you
Her villain in this bright of his grief,
And this fit force and feeling out.

GLOUCESTER:
And therefore comes him with your trights.

KING EDWARD IV:
If he will you take more deserved.

LADY GREY:
Pardon the Tower wise, Pompey, my lord.

LADY ANNE:
I shall the hear of that doth
============================================================

>>> Training LIF-fixed...
Device: cpu
Ember: 10.65M parameters (LIF=ON)
[LIF-fixed] iter     0 | train 4.2882 | val 4.2811 | 27.7s
[LIF-fixed] iter   500 | train 1.9238 | val 2.0270 | 820.4s
[LIF-fixed] iter  1000 | train 1.4142 | val 1.6261 | 1611.0s
[LIF-fixed] iter  1500 | train 1.2592 | val 1.5230 | 2401.8s
[LIF-fixed] iter  1999 | train 1.1723 | val 1.4952 | 3191.2s

============================================================
[LIF-fixed] Training complete in 3192.8s
[LIF-fixed] Best val loss: 1.4952

--- Generated sample ---

And swing with no more than with me;
And than I, is a man pride miless to see him.

Second Murderer:
'Tis all the sangs you shall hear my sweet shame
Against your eyes and all your confess,
That as the find, feat of a while,
And so you would not: I must do it stood
A nature for war back.

Second Murderer:
You have well the cousin of this will I sleep.
Do you the devil of heaven, you were more,
Come you to come any your royal fearful souls,
And the other drop of your suit, seize it is ever
Like 
============================================================

--- Learned LIF parameters (per-head) ---

>>> Training LIF-learnable...
Device: cpu
Ember: 10.65M parameters (LIF=ON)
[LIF] iter     0 | train 4.2882 | val 4.2811 | 27.5s
[LIF] iter   500 | train 1.9535 | val 2.0564 | 878.2s
[LIF] iter  1000 | train 1.4061 | val 1.6258 | 1723.9s
[LIF] iter  1500 | train 1.2487 | val 1.5089 | 2570.1s
[LIF] iter  1999 | train 1.1565 | val 1.4694 | 3416.5s

============================================================
[LIF] Training complete in 3418.3s
[LIF] Best val loss: 1.4694

--- Generated sample ---

And shall we are alone.

KING EDWARD IV:
Be mad, good lord, what ill serves all;
Or shall the body of this love and less.

LORD WILLOUGHBY:
And, marry, ere I am good on his mother,
His noble life shall be worn in the world
For the fair word in the hour of the story;
Nor no prince are felting of the very ears
Than the devil of his friend, which in his power,
That it were a cause, you were more dead;
But then they have to stronge them soul to the deep is
Burns to death, in their affections made
B
============================================================

--- Learned LIF parameters (per-head) ---
  transformer.h.0.attn.threshold: [-0.0002, 0.0003, 1.1394, -0.0002, -0.0001, 0.7493]
  transformer.h.0.attn.leak: [3.2400, 2.8430, 1.0581, 2.9741, 3.1601, 1.5561]
  transformer.h.0.attn.steepness: [0.7814, 1.2862, 2.8661, 1.0486, 0.8359, 2.5242]
  transformer.h.1.attn.threshold: [0.0001, 0.0010, -0.0004, 0.0002, 0.8024, -0.0005]
  transformer.h.1.attn.leak: [3.1167, 2.6882, 2.8086, 3.0157, 1.5344, 2.9989]
  transformer.h.1.attn.steepness: [0.8883, 1.3218, 1.2279, 1.0041, 2.4103, 1.0013]
  transformer.h.2.attn.threshold: [-0.0015, 0.3484, 0.0007, -0.0015, -0.0007, 0.0142]
  transformer.h.2.attn.leak: [3.0105, 1.9984, 2.8008, 2.5291, 2.3496, 2.3547]
  transformer.h.2.attn.steepness: [1.0000, 1.9174, 1.2729, 1.4903, 1.6413, 1.6582]
  transformer.h.3.attn.threshold: [-0.0037, 0.0003, -0.0004, -0.0006, 0.0002, 0.0009]
  transformer.h.3.attn.leak: [2.4964, 2.9405, 2.6475, 2.5489, 2.7624, 2.7120]
  transformer.h.3.attn.steepness: [1.4705, 1.0897, 1.3814, 1.5332, 1.2873, 1.2956]
  transformer.h.4.attn.threshold: [-0.0001, -0.0007, 0.0043, 0.0008, -0.0023, -0.0480]
  transformer.h.4.attn.leak: [2.7050, 2.5609, 2.4187, 2.5232, 2.6481, 2.3246]
  transformer.h.4.attn.steepness: [1.3283, 1.4813, 1.5900, 1.4996, 1.3749, 1.6800]
  transformer.h.5.attn.threshold: [0.0505, -0.0003, -0.0430, 0.0005, 0.0014, 0.0030]
  transformer.h.5.attn.leak: [2.3932, 2.4539, 2.3215, 2.3486, 2.7222, 2.3881]
  transformer.h.5.attn.steepness: [1.5535, 1.5515, 1.6328, 1.6183, 1.3262, 1.5947]

>>> Training LIF-refractory...
Device: cpu
Ember: 10.65M parameters (LIF=ON)
[LIF-refrac] iter     0 | train 4.2882 | val 4.2811 | 28.0s
[LIF-refrac] iter   500 | train 1.9076 | val 2.0231 | 882.3s
[LIF-refrac] iter  1000 | train 1.3864 | val 1.6008 | 1736.4s
[LIF-refrac] iter  1500 | train 1.2449 | val 1.5088 | 2592.6s
[LIF-refrac] iter  1999 | train 1.1582 | val 1.4676 | 3446.5s

============================================================
[LIF-refrac] Training complete in 3448.2s
[LIF-refrac] Best val loss: 1.4676

--- Generated sample ---

KING EDWARD IV:
Who should not have done, but swear a bower which
look to see him.

Second Murderer:
'Tis thou seest him.

PRINCE EDWARD:
See thou wilt end to my service for her.

LADY ANNE:
I spake her hath feat to a white,
And so cursed, Abria: thou for the story things
That conspires the king's sun, we are barren both;
And his mother were to blood to thy.

BUCKINGHAM:
O, within me, this is dead; you tell me infruite
Of the Duke of Norfolk, and God king Edward's good:
And make her fell me tha
============================================================

--- Learned LIF parameters (per-head) ---
  transformer.h.0.attn.threshold: [-0.4633, -0.0002, 1.1406, 0.0008, 0.0004, -0.0001]
  transformer.h.0.attn.leak: [1.5478, 3.2144, 1.0551, 3.2348, 2.9756, 3.2613]
  transformer.h.0.attn.steepness: [2.5880, 0.7797, 2.8689, 0.7651, 1.0244, 0.8924]
  transformer.h.0.attn.refractory_strength: [-0.7487, -1.0688, -0.9928, -1.1014, -1.0521, -0.5919]
  transformer.h.0.attn.cross_layer_weight: [-2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000]
  transformer.h.1.attn.threshold: [-0.0003, -0.0007, 0.0820, -0.0022, 0.0024, -0.0022]
  transformer.h.1.attn.leak: [2.8535, 2.7260, 2.4369, 2.8626, 2.6674, 2.6850]
  transformer.h.1.attn.steepness: [1.1259, 1.2986, 1.5787, 1.1416, 1.3333, 1.3325]
  transformer.h.1.attn.refractory_strength: [-1.0109, -1.0562, -0.6383, -0.7498, -1.1296, -1.1501]
  transformer.h.1.attn.cross_layer_weight: [-1.8449, -1.6563, -1.7767, -1.8416, -1.6323, -1.7139]
  transformer.h.2.attn.threshold: [0.0002, 0.0013, -0.0000, 0.0001, 0.8007, -0.4776]
  transformer.h.2.attn.leak: [2.8015, 3.0188, 3.1314, 2.9091, 1.4965, 1.8877]
  transformer.h.2.attn.steepness: [1.2155, 1.0116, 0.8737, 1.0857, 2.3737, 2.0750]
  transformer.h.2.attn.refractory_strength: [-0.6113, -1.0980, -1.3447, -0.9592, -0.3188, -0.4941]
  transformer.h.2.attn.cross_layer_weight: [-1.3871, -1.5241, -1.7439, -1.7619, -1.0204, -1.2871]
  transformer.h.3.attn.threshold: [-0.0002, 0.0006, -0.0005, -0.0004, 0.0000, 0.0000]
  transformer.h.3.attn.leak: [2.7366, 2.3965, 2.5587, 2.8698, 2.4636, 2.2916]
  transformer.h.3.attn.steepness: [1.3052, 1.6138, 1.5423, 1.1466, 1.5506, 1.7616]
  transformer.h.3.attn.refractory_strength: [-0.5827, -0.5317, -0.5297, -1.2924, -0.4699, -0.3955]
  transformer.h.3.attn.cross_layer_weight: [-1.1146, -1.0629, -1.1483, -1.7600, -1.0011, -0.9377]
  transformer.h.4.attn.threshold: [0.2450, -0.0005, -0.1292, -0.0002, 0.1819, 0.0002]
  transformer.h.4.attn.leak: [2.1272, 2.3729, 2.1602, 2.6759, 2.0929, 2.2893]
  transformer.h.4.attn.steepness: [1.8084, 1.6427, 1.8282, 1.3366, 1.8336, 1.7471]
  transformer.h.4.attn.refractory_strength: [-0.5501, -0.7656, -0.6583, -1.1365, -1.5962, -0.4267]
  transformer.h.4.attn.cross_layer_weight: [-1.0546, -1.2589, -1.1068, -1.5641, -1.1200, -0.9902]
  transformer.h.5.attn.threshold: [-0.0005, -0.0023, 0.0006, -0.0004, -0.0003, -0.0011]
  transformer.h.5.attn.leak: [2.6831, 2.5054, 2.5271, 2.6553, 2.7848, 2.6659]
  transformer.h.5.attn.steepness: [1.3237, 1.4829, 1.4673, 1.3583, 1.2582, 1.3615]
  transformer.h.5.attn.refractory_strength: [-0.8676, -0.8623, -0.8539, -0.8922, -1.1520, -0.9198]
  transformer.h.5.attn.cross_layer_weight: [-1.7571, -1.4988, -1.4614, -1.6346, -2.0742, -1.6240]

>>> Training Qwen-gate...
Device: cpu
Ember: 11.53M parameters (LIF=OFF)
[Qwen-gate] iter     0 | train 4.1787 | val 4.1765 | 17.0s
[Qwen-gate] iter   500 | train 1.6123 | val 1.7990 | 702.2s
[Qwen-gate] iter  1000 | train 1.3768 | val 1.6100 | 1379.1s
[Qwen-gate] iter  1500 | train 1.2451 | val 1.5250 | 2051.9s
[Qwen-gate] iter  1999 | train 1.1532 | val 1.4942 | 2727.0s

============================================================
[Qwen-gate] Training complete in 2728.3s
[Qwen-gate] Best val loss: 1.4942

--- Generated sample ---


Roman:
O, though I lade me alone.

Second Murderer:
Well, come, myself are none.

First Murderer:
Well, what, my lord, that news?

First Murderer:
Forsake, good lord, I come.

Second Murderer:
You must leave you well: tell me, see, the gentleman of Perdita, peace.

Second Murderer:
What is the mattery had an old dangerous report.

Second Murderer:
Sirs, sir, and he hath he we no.

Second Murderer:
Go, before you perfects him: he's when your head.

Second Murderer:
Nay, is not the fortune be di
============================================================

============================================================
ABLATION RESULTS
============================================================
  Standard              val_loss=1.4923  time=2562.8s  diff=+0.00%
  LIF-fixed             val_loss=1.4952  time=3192.8s  diff=+0.20%
  LIF-learnable         val_loss=1.4694  time=3418.3s  diff=-1.53%
  LIF-refractory        val_loss=1.4676  time=3448.2s  diff=-1.65%
  Qwen-gate             val_loss=1.4942  time=2728.3s  diff=+0.13%

Best: LIF-refractory (val_loss=1.4676)
